
@inproceedings{mikolov_distributed_2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  year = {2013},
  pages = {3111--3119}
}

@article{gutmann_noise-contrastive_2012,
  title = {Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics},
  volume = {13},
  number = {1},
  journal = {The Journal of Machine Learning Research},
  author = {Gutmann, Michael U. and Hyv{\"a}rinen, Aapo},
  year = {2012},
  pages = {307--361}
}

@inproceedings{le_distributed_2014,
  title = {Distributed Representations of Sentences and Documents},
  booktitle = {{{arXiv}} Preprint {{arXiv}}:1405.4053},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014}
}

@inproceedings{rehurek_software_2010,
  address = {Valletta, Malta},
  title = {Software {{Framework}} for {{Topic Modelling}} with {{Large Corpora}}},
  language = {English},
  booktitle = {Proceedings of the {{LREC}} 2010 {{Workshop}} on {{New Challenges}} for {{NLP Frameworks}}},
  publisher = {{ELRA}},
  author = {{\v R}eh\r{u}{\v r}ek, Radim and Sojka, Petr},
  month = may,
  year = {2010},
  pages = {45--50}
}

@inproceedings{kingma_adam:_2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  shorttitle = {Adam},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2015)},
  author = {Kingma, Diederik and Ba, Jimmy},
  year = {2014}
}

@article{dwork_preserving_2014,
  title = {Preserving {{Statistical Validity}} in {{Adaptive Data Analysis}}},
  abstract = {A great deal of effort has been devoted to reducing the risk of spurious scientific discoveries, from the use of sophisticated validation techniques, to deep statistical methods for controlling the false discovery rate in multiple hypothesis testing. However, there is a fundamental disconnect between the theoretical results and the practice of data analysis: the theory of statistical inference assumes a fixed collection of hypotheses to be tested, or learning algorithms to be applied, selected non-adaptively before the data are gathered, whereas in practice data is shared and reused with hypotheses and new analyses being generated on the basis of data exploration and the outcomes of previous analyses. In this work we initiate a principled study of how to guarantee the validity of statistical inference in adaptive data analysis. As an instance of this problem, we propose and investigate the question of estimating the expectations of \$m\$ adaptively chosen functions on an unknown distribution given \$n\$ random samples. We show that, surprisingly, there is a way to estimate an exponential in \$n\$ number of expectations accurately even if the functions are chosen adaptively. This gives an exponential improvement over standard empirical estimators that are limited to a linear number of estimates. Our result follows from a general technique that counter-intuitively involves actively perturbing and coordinating the estimates, using techniques developed for privacy preservation. We give additional applications of this technique to our question.},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.2664},
  primaryClass = {cs},
  journal = {arXiv:1411.2664 [cs]},
  author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
  month = nov,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Learning}
}

@article{shalev-shwartz_learnability_2010,
  title = {Learnability, Stability and Uniform Convergence},
  volume = {11},
  number = {Oct},
  journal = {Journal of Machine Learning Research},
  author = {Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  year = {2010},
  pages = {2635--2670}
}

@misc{mikolov_distributed_2014,
  title = {"{{Distributed Representations}} of {{Sentences}} and {{Documents}}" {{Code}}? - {{Google Groups}}},
  howpublished = {\url{https://groups.google.com/forum/\#!msg/word2vec-toolkit/Q49FIrNOQRo/DoRuBoVNFb0J}},
  author = {Mikolov, Tomas},
  month = sep,
  year = {2014}
}

@misc{mohr_gensim_2017,
  title = {Gensim Doc2vec \& {{IMDB}} Sentiment Dataset},
  abstract = {gensim - Topic Modelling for Humans},
  howpublished = {\url{https://github.com/RaRe-Technologies/gensim/blob/becc6d3be63627ff7b05906fca8a113ab1e128a0/docs/notebooks/doc2vec-IMDB.ipynb}},
  journal = {GitHub},
  author = {Mohr, Gordon},
  year = {2017}
}

@inproceedings{arora_rand-walk:_2015,
  title = {{{RAND}}-{{WALK}}: {{A Latent Variable Model Approach}} to {{Word Embeddings}}},
  shorttitle = {{{RAND}}-{{WALK}}},
  abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of\textasciitilde$\backslash$citet\{mnih2007three\}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by\textasciitilde$\backslash$citet\{mikolov2013efficient\} and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03520},
  primaryClass = {cs, stat},
  booktitle = {{{arXiv}}:1502.03520 [Cs, Stat]},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Statistics - Machine Learning},
  annote = {Comment: to appear in Transactions of the Association for Computational Linguistics (TACL)}
}

@inproceedings{mnih_scalable_2009,
  title = {A Scalable Hierarchical Distributed Language Model},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mnih, Andriy and Hinton, Geoffrey E.},
  year = {2009},
  pages = {1081--1088}
}

@article{dyer_notes_2014,
  title = {Notes on {{Noise Contrastive Estimation}} and {{Negative Sampling}}},
  abstract = {Estimating the parameters of probabilistic models of language such as maxent models and probabilistic neural models is computationally difficult since it involves evaluating partition functions by summing over an entire vocabulary, which may be millions of word types in size. Two closely related strategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al., 2012; Goldberg and Levy, 2014)---have emerged as popular solutions to this computational problem, but some confusion remains as to which is more appropriate and when. This document explicates their relationships to each other and to other estimation techniques. The analysis shows that, although they are superficially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while negative sampling is best understood as a family of binary classification models that are useful for learning word representations but not as a general-purpose estimator.},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.8251},
  primaryClass = {cs},
  journal = {arXiv:1410.8251 [cs]},
  author = {Dyer, Chris},
  month = oct,
  year = {2014},
  keywords = {Computer Science - Learning},
  annote = {Comment: 4 pages}
}

@inproceedings{maas-EtAl:2011:ACL-HLT2011,
  address = {Portland, Oregon, USA},
  title = {Learning {{Word Vectors}} for {{Sentiment Analysis}}},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  month = jun,
  year = {2011},
  pages = {142--150}
}


