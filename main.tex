\PassOptionsToPackage{pdfpagelabels=false}{hyperref}
\documentclass{article}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[titletoc,title]{appendix}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}

\makeatletter
\newcommand*{\Appendixautorefname}{Appendix}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newsavebox\CBox\def\textBF#1{\sbox\CBox{#1}\resizebox{\wd\CBox}{\ht\CBox}{\textbf{#1}}}\parindent=0pt
\sisetup{exponent-product = \cdot}
\setlength{\parskip}{0.8em}
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother

\begin{document}

\title{Reproduction of Distributed Representations of Sentences and Documents}

\author{
  \textbf{Xingyi Xu} \\ University of Melbourne \\ \texttt{stevenxxiu@gmail.com} \and
  \textbf{Jey Han Lau} \\ IBM Research \\ \texttt{depthchargex@gmail.com} \and
  \textbf{Timothy Baldwin} \\ University of Melbourne \\ \texttt{tb@ldwin.net}
}
\maketitle

\begin{abstract}
Neural network models can be difficult to reproduce due to its many hyperparameters, initialization and training details. We attempted to reproduce the results of a paper ``Distributed Representations of Sentences and Documents'' for the imdb sentiment analysis dataset, using \texttt{tensorflow}. We could not obtain the paper's claim of an 7.42\% error rate, and instead we managed 11.59\%. This is however very close to the results of \texttt{gensim}, another implementation of the paper, in a similar setting, which gives an error rate of 12.04\%.
\end{abstract}

\section{Introduction}
The recent resurgence in AI is in large part due to the many successful neural network models. These models often have many hyperparameters, and can be initialized and trained in multiple ways. However, the focus on many papers has been just the model itself, with a minimal or neglected description of the details, possibly due to the fact that different initilization and training schemes can still achieve similar results. Hence it is useful to investigate whether we can reproduce neural network papers using its description alone, and if not, what can be done to improve reproducibility.

\texttt{word2vec} \citep{mikolov_distributed_2013} was one of the first papers in which words were successfully represented as dense embedding vectors of fixed size. The model was simple -- a sliding window was used on both sides of a central word to predict it. As to be expected, words with similar meaning were found to be grouped together in the vector space since they will predict similar words. Intriguingly, there was also meaning in the addition and subtraction of vectors, where \textit{man - woman + king $\approx$ queen}. \cite{arora_rand-walk:_2015} have come up with a theoretical explanation for this phenomena.

As an extension of the methods used in \texttt{word2vec} to documents, \texttt{doc2vec} \citep{le_distributed_2014} introduced 2 models. One, \texttt{pvdm}, uses a concatenated or averaged vector of the document embedding plus all the embeddings of words in the output word's context to predict the output word. In contrast to \texttt{word2vec}, the conditional includes the entire context (and document) instead of just the words in the context alone. Another, \texttt{dbow}, uses the document embedding only so does not require contexts nor does it give word embeddings.

Despite the document vectors not being trained on sentiment information, \cite{le_distributed_2014} report that their model was able to achieve a state-of-the-art result of a 7.42\% error rate. However, we could only produce an error rate of 11.59\%.\footnote{Our source code is available at \url{https://github.com/stevenxxiu/dist_rep_sent_doc/}.} This does not beat the best model listed for comparison, which achieves an error rate of 8.78\%.

Of worth noting is that the second author of the paper, Tomas Mikolov, also thinks that the paper is unreproducible, commenting in a thread in the word2vec Google group that he gets error rates of 9.4\% to 10\% \citep{mikolov_distributed_2014} . This higher result can be achieved if we use negative sampling instead of hierarchical softmax, and if the test documents are included in the training set instead of their vectors being inferred \citep{mohr_gensim_2017}, however this strays somewhat from the model as described in the paper.

In the following sections, we give a detailed description of our interpretation of the \texttt{doc2vec} models, any ambiguities in the paper which slowed down our implementation, and a report of how we managed to implement the model.

\section{Models}
The idea of \texttt{doc2vec} is to predict all the words within a document, given an input vector which contains the document embedding. The hope is that the information from the document embedding can be used to predict its words well, so that the document embedding can capture some meaning in the document. We first describe the \texttt{doc2vec} model for an arbitrary input vector $u_{d,t}$, which depends on the document $d$ and output word position $t$, then show what $u_{d,t}$ is respectively for \texttt{pvdm} and \texttt{dbow}.

Given a document $d\in\mathcal{D}$ for the document collection $\mathcal{D}$, the words in $d$, $w_{d_1},w_{d_2},...,w_{d_T}$, the \texttt{doc2vec} model attempts to maximize the log probability:
    \[\sum_{d\in\mathcal{D}}\sum_{t=1}^{T_d} \log p(w_{d_t}|u_{d,t})\]

To represent $p(w_t|u_{d,t})$, we use a dense layer plus a softmax function, which is rather standard in neural nets classifiers:
    \[p(w_O|u_{d,t})=\frac{\exp(v_{w_O}'^T u_{d,t})}{\sum_{w=1}^W \exp(v_w'^T u_{d,t})}\]
here $W$ is the vocabulary size, and $v_w'$ are the scalar sums of the input for the output class (here a word) $w'$. Note there is no bias in the dense layer. However, the denominator is difficult to compute due to the large $W$ involved. Possible approaches (those used in \texttt{word2vec}) include:
\begin{itemize}
    \item Substituting this with the hierarchical softmax \citep{mnih_scalable_2009}.
    \item Using Noise Contrastive Estimation (NCE) \citep{gutmann_noise-contrastive_2012}, a method which gives an objective to allow for the optimization of parameters for an unnormalized probability distribution, here again we jointly optimize all conditional distributions by summing the objectives.
    \item Negative Sampling, an approximation of NCE, which is derived by assuming the noise distribution is the discrete uniform distribution over all $W$ words and $W$ negative samples are used per word, then using a different noise distribution and number of negative samples during training time \citep{dyer_notes_2014}.
\end{itemize}

\texttt{doc2vec} uses hierarchical softmax. This is a binary tree, where there are $W$ leaves each corresponding to an output word. Each node $n$ in the tree is an independent binary classifier, with an associated weight vector $v_n'$, and all nodes have the same input vector. So supposing the path of $w_O$ from the root is $n_1,...,n_L,w_O$, and $c_1,...,c_L$ such that $c_j$ is $-1$ if $n_{j+1}$ is the left child of ${n_j}$ and $1$ if it is the right child. Then we have:
    \[p(w_O|u_{d,t})=\prod_{j=1}^{L}\sigma(c_j\cdot v_{n_j}'^T u_{d,t})\]
where $\sigma$ is the sigmoid function. It is not hard to see that this sums to $1$ by assigning each node a probability defined as the sum of probabilities of it's descendants. The tree used in hierarchical softmax is a binary Huffman tree, to assist in computation since short codes are assigned to frequent words.

Now, for \texttt{pvdm}, given the output word $w_O$, $u_{d,t}$ is the average or concatenation of the document embedding $v_d$ and $v_{w_1},...,v_{w_c}$, where $c$ is the context length, and $w_1,..,w_c$ precede $w_O$. If $w_O$ is the first word so that $w_1$ might not exist, we pad the document to the left so all the words which do not exist are assigned the special \texttt{null} word. This way $u_{d,t}$ is always the same length. For \texttt{dbow}, $u_{d,t}$ is just the document embedding $v_d$.

\subsection{Classification}
To apply the model to the classification of the IMDB sentiment analysis dataset, the document vectors are fed into a neural network with a dense layer and a final softmax classification layer.

\subsection{Ambiguities in the paper}
Hierarchical softmax sometimes has a bias \citep{mnih_scalable_2009}. It is not clear whether this is the case in the original paper, since the description of the softmax in their Equation (1) has a bias, however that of the \texttt{word2vec} paper does not. There is no mention of what happens in the case of hierarchical softmax. We assume that there is no bias, since this paper is an extension of \texttt{word2vec}.

The overall objective is not described in the original paper. Usually it is the case that all objectives are summed for each training example, but here there are two levels of training examples -- that of documents and that of words within them. So another possible objective, normalizing for document length is:
    \[\sum_{d\in\mathcal{D}}\frac{1}{T_d}\sum_{t=1}^{T_d} \log p(w_{d_t}|u_{d,t})\]
In our case we just assume that it is a sum for simplicity.

For the classification network, the neural network is not very well described. We assume it is the most common neural network, with dense layers with ReLU activation units, and a final softmax classification layer including biases, and do not introduce any regularization.

\section{Experimental results}
In our reproduction of the original methods of the paper, we could not reproduce the paper's claim of an 7.42\% error rate, and instead we managed 11.59\%. Under a similar setting, using another implementation, \texttt{gensim} \citep{rehurek_software_2010}, under their tuned hyperparameters we arrived at a similar error rate of 12.04\%.\footnote{Details regarding \texttt{gensim} are in \autoref{sec:gensim}.}

We initially did not succeed in getting an error rate above 50\%, so we used \texttt{gensim}'s source code for guidance. The key changes which allowed us to do so included good initialization of the weights as described below, and using the summed loss in each mini-batch instead of the mean.

\subsection{Ambiguities common to most neural network papers}
There are some underspecified parts of the model which are also common to most neural network papers. In particular, the random initializations of the parameters, here word embedding and softmax weights are not specified. We found that these were quite important to the model, as our initial attempts did not work. Using the initializations of \texttt{gensim}, that of $U(-1/d, 1/d)^d$ for word embeddings for embedding dimension size $d$ and $0$ for the hierarchical softmax weights worked.

\subsection{Ambiguities in the paper}
The training is not described in the original paper. This includes the training objective, the gradient descent method, and whether any batching was involved. For the objective, we used the most common setup of randomly shuffling all training examples, training a batch of that per time step with a batch size of $2048$.

We first attempted to use stochastic gradient descent by itself. We found that the sum loss for each batch instead of the mean loss appeared to affect the learning rates much less depending on different batch sizes. To explain this, suppose the loss function is $l$ with single weight $w$, then the summed batch update for weight $w$ is $\partial\sum_{i=1}^n l(x_i, w) / \partial{w} = \sum_{i=1}^n \partial l(x_i, w)/\partial(w)$. So when using the mean, the single gradient step uses the mean gradient of all training instances. On the contrary, if the weight is different for each training instance, we have $\partial\sum_{i=1}^n l(x_i, w_i) / \partial{w_i} = \partial l(x_i, w_i)/\partial(w_i)$, so it makes less sense to use the mean. Since the model makes use of word embeddings, the model is closer to the latter, since most batches do not share the same words and documents, hence the same parameters for the word and document embeddings.

However, stochastic gradient descent still appeared to cause some rare parameters to have very large gradients and to converge very slowly (especially the \texttt{null} word since it appears more frequently in each batch), causing very large losses for some batches. Clipping the gradients stopped divergence, however there was the additional issue that large batch sizes were less performant. So we instead opted to use \texttt{Adam} \citep{kingma_adam:_2014}, which scales gradients automatically and performs better for rare weights. For efficiency, in our particular case we use a lazy version of Adam which for sparse parameters only updates its moving average accumulators for parameters present in the current batch, instead of all parameters.\footnote{\texttt{LazyAdamOptimizer} in \texttt{tensorflow}.}

What the context window size, $c$ actually is can be missed. It might be unclear from the start, but in the later sections the paper states ``To predict the 10-th word, we concatenate the paragraph vectors and word vectors'', so in the IMDB case, $c=9$.

It is not described how unknown words during validation are handled. We followed \texttt{gensim} in omitting unknown and words below a frequency threshold from the document entirely, to reduce memory consumption.

The method of inference is described, but is rather brief. In more detail, our optimization setup was identical for training and inference phases, with identical learning rates. We still randomly initialize and update the test document vectors in the same way as the training phase, but all other parameters are copied from the end of the training phase and kept fixed.

\subsection{Techniques only mentioned in the \texttt{word2vec} paper}
Another technique we found to be useful that was only mentioned in the \texttt{word2vec} paper was frequent word sub-sampling. For each epoch, this preprocessed each document first by supposing the probability of a word $w$ being included in the document is $\max\{(t/f(w))^{1/2}, 1\}$, where $f(w)$ is the empirical probability of the word ${w}$ in the training set, and $t$ is some chosen threshold. Windows are then extracted from the documents in a separate pass. The idea is that frequent words may provide less information value than rarer words, but contribute disproportionately to the loss function, thus resulting in less good word vectors for infrequent words. We found this to be useful for both the \texttt{pvdm} and \texttt{dbow} models.

\subsection{Hyperparameter tuning}
We tuned out own hyperparameters, since some hyperparameters such as the number of epochs were missing from the paper, and in addition we used frequent word sub-sampling.

The \texttt{imdb} dataset does not include a validation set. We constructed our validation set similarly to how the test set was constructed. In order for classes to remain balanced, we took 20\% of the positive and negative training examples. In order to avoid performance improvements from memorizing rare movie-unique terms and their associated labels from the training set, we ensure that the validation and training sets have no movie in common \citep{maas-EtAl:2011:ACL-HLT2011}. This resulted in us taking the first $10004$ positive and first $10001$ reviews out of $12500$ from the positive and negative classes respectively.

The process of hyperparameter tuning was performed via random search. Our search space is detailed in \autoref{table:hyper}. Some of the larger settings for the \texttt{pvdm} concat model resulted in out-of-memory errors, but we do not believe this to affect the results since the best setting found had a rather small embedding size. For each run, we picked the best result from all logistic regression epochs.

\begin{table}[htbp]\centering
\setlength\tabcolsep{2pt}
\begin{tabular}{|l|l|}
    \hline
    Parameter               & Search space \\ \hline
    Embedding size          & 50, 100, 150, ..., 400 \\
    Window size             & 5, 6, 7, ..., 12 \\
    Min freq                & 0, 1, 2, ..., 5 \\
    Freq word sub-sample    & 1, \num{1e-1}, \num{1e-2}, ..., \num{1e-5} \\
    Train LR                & 0.1, 0.05, 0.025, 0.01, ..., 0.0001 \\
    Inference LR            & 0.1, 0.05, 0.025, 0.01, ..., 0.0001 \\
    Train epoch size        & 1, 2, 3, ..., 50 \\
    Inference epoch size    & 1, 2, 3, ..., 50 \\
    \hline
\end{tabular}
\caption{Hyperparameter search spaces.}
\label{table:hyper}
\end{table}

\subsection{Results}
The non-concatenated results are in \autoref{table:doc_emb}. We can see that the \texttt{dbow} model performs best, concat and mean perform similarly, and that different methods have very different embedding sizes, perhaps due to data size limitations. In \autoref{table:classification}, we find that neural nets do not help due to overfitting, and logistic regression is sufficient. Additionally, we see that \texttt{dbow} alone is better than any form of concatenation. This makes some sense when we view \texttt{pvdm} as an extension to the \texttt{dbow} model, with the additional context of words in each window.

\begin{table}[htbp]\centering
\setlength\tabcolsep{2pt}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|}
    \hline
    Method & \specialcell{Embedding \\ size} & \specialcell{Minimum \\ word freq} & \specialcell{Freq word \\ sub-sample} & \specialcell{Window \\size} & \specialcell{Train \\ LR} & \specialcell{Inference \\ LR} & \specialcell{Train \\ epochs} & \specialcell{Inference \\ epochs} & \specialcell{Error rate} \\ \hline
    pvdm concat & 50  & 2 & \num{1e-3} & 7 & 0.0005 & 0.00025 & 46 & 42 & 0.152280 \\
    pvdm mean   & 150 & 4 & \num{1e-3} & 8 & 0.0025 & 0.0001  & 26 & 49 & 0.152520 \\
    dbow        & 400 & 0 & \num{1e-1} & - & 0.001  & 0.0001  & 12 & 18 & \textBF{0.115880} \\
    \hline
\end{tabular}
\caption{\texttt{pvdm} and \texttt{dbow} results for logistic regression classification. All results used a batch size of $2048$. Logistic regression had a learning rate of $0.01$ with Adam's default settings, a batch size of $2048$, and $100$ epochs. We took the result of the epoch with the best validation results. Some \texttt{gensim} modifications we experimented with are also included for comparison.}
\label{table:doc_emb}
\end{table}

\begin{table}[htbp]\centering
\begin{tabular}{|l|l|r|r|}
    \hline
    Method     & Document vectors               & Layer sizes & Error rate \\ \hline
    logreg     & pvdm concat                    & -           & 0.152280 \\
    logreg     & pvdm mean                      & -           & 0.152520 \\
    logreg     & dbow                           & -           & \textBF{0.115880} \\
    logreg     & pvdm concat + pvdm mean        & -           & 0.143640 \\
    logreg     & pvdm concat + dbow             & -           & 0.119440 \\
    logreg     & pvdm mean + dbow               & -           & 0.117920 \\
    logreg     & pvdm concat + pvdm mean + dbow & -           & 0.120480 \\
    neural net & pvdm concat                    & 10          & 0.151040 \\
    neural net & pvdm mean                      & 6           & 0.285720 \\
    neural net & dbow                           & 30          & 0.125000 \\
    \hline
\end{tabular}
\caption{Classification results comparing modes of concatenation, and logistic regression vs neural nets. Training had a learning rate of $0.01$ with Adam's default settings, a batch size of $2048$, and $100$ epochs. The neural net layer size search was performed over $2, 3, 4, ..., 10, 20, 30, ...$ until the embedding size. We took the result of the epoch with the best validation results.}
\label{table:classification}
\end{table}

\section{Conclusion}
We have described the process in which we attempted to reproduce the paper ``Distributed Representations of Sentences and Documents''. The main difficulties we encountered included the ommitted description of the initialization of neural network weights, objective where multiple batches are involved, the gradient descent specifics, and techniques from \texttt{word2vec} which gave a significant improvement to the results but were probably ommitted from the paper. We hope that our findings can guide future paper authors in the area of deep learning to describe their models in a clearer fashion so that they are more easily reproducible.

\begin{appendices}
\section{\texttt{gensim}} \label{sec:gensim}
\texttt{gensim} is another implementation of \texttt{doc2vec}, which is a python port of Mikolov's C implementation of \texttt{doc2vec}. Here we list how we used this implementation for comparison, and some additional differences \texttt{gensim}'s implementation (inroduced in Mikolov's C implementation) has compared to the paper's model.

\subsection{Modifications to the \texttt{gensim} notebook}
We use a slightly different setup to the example \texttt{gensim} notebook \citep{mohr_gensim_2017}. The notebook selects the best epoch based on the test set, which perhaps could be interpreted as early stopping but since this is not the classification stage this is a stretch, so we use the result after all epochs are run. The notebook also includes the test document for training word vectors, which we exclude entirely and use inferred vectors only during testing. We also use hierarchical softmax instead of negative sampling.

\subsection{Differences in \texttt{gensim}}
Besides the slightly different setup in the \texttt{gensim} notebook, the \texttt{gensim} model implementation itself also presents some differences not mentioned in any paper.

There is the option to remove infrequent words. Words below a frequency threshold are excluded as if they were not in the document at all, during training and inference. We used this to reduce memory consumption.

It alters the frequent word subsampling formula. The probability of a word being included in the document in \texttt{word2vec} is $P(w)=\max\{(t/f(w))^{1/2}, 1\}$. The altered formula is $P(w)=\max\{(t/f(w))^{1/2} + {t/f(w)}, 1\}$. Since for most words $t/f(w)\approx 0$, the difference between the two formulas are not great. We tried the altered formula but did not obtain better results than using the original.

Reduced window sampling, where instead of using $c$, we use $c'\sim\mathcal{U}\{0,c\}$. This is used in in the \texttt{pvdm} average model and in the \texttt{dbow} model where skip-gram is used. This was not used in the \texttt{pvdm} concat model, presumably because the model required $u_{d,t}$ to have fixed in dimension. We did not experiment with this due to it not being mentioned in either paper.

The \texttt{pvdm} model predicts the center word instead of last word, similar to the \texttt{word2vec} skip-gram model. The window sizes on the left and right are required to be the same. We found that predicting the end word gave better results, while the word embeddings are subjectively similarly good.

The \texttt{pvdm} average model has no pre-padding of \texttt{null} words, although it is not clear whether the original paper did this or not. We performed pre-padding.

\texttt{gensim} does not use mini-batching. In order to make use of parallelism, the documents are first shuffled then queued, each training thread takes a document from the queue, and trains on all windows for a single document from left to right. The threads read and update the weights without any form of synchronization. If we assume that all threads run at identical speed and start at identical times, i.e. at each time step, $n$ threads read the parameters, then compute the updates, then update the parameters through atomic vector addition, this is equivalent to mini-batching with a batch size of $n$, with summed loss. Examples in the mini-batch are the same as in the case of \texttt{gensim}'s threads, note that it always contains windows from different documents.

\end{appendices}

\bibliography{main}{}
\bibliographystyle{apalike}

\end{document}
